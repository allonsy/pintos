			+--------------------+
			|        CS 140      |
			| PROJECT 1: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Alec Snyder <alsnyder@uchicago.edu>
Alejandro Younger <acy1@uchicago.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

GLOBALS

/* list holds threads slept by timer_sleep */
static struct list sleep_wait_list;

ADDTIONAL struct thread MEMBERS

struct list_elem sleepelem;  /* List for threads put to sleep by timer_sleep */
  
int64_t wakeup_time; /* holds tick value after which thread can be awoken */

struct semaphore timer_sema; /* indicates readiness for waking */

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

First the thread compute the time (in ticks) after which the thread can wake up
and sets its wakeup_time to this value. 
Based on this, it insert itself into the list of sleeping threads, ordered 
increasing in terms of wakeup_time. 
Then it puts itself to sleep by calling sema_down on its timer_sema.
In the timer interrupt handler, we iterate over the list and call
sema_up on &t->timer_sema for each t with wakeup_time <= timer_ticks()

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

The list is sorted in increasing order, and the iteration loop breaks
as soon as it sees a thread whose wakeup_time is still in the future.


---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

Interrupts are disabled in timer_sleep.
Notably, this is a good use of disabling interrupts as timer_sleep()
modifies a global list.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Interrupts are disabled in timer_sleep, so this cannot happen.
Notably, this is a good use of disabling interrupts as timer_sleep()
modifies a global list.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?
We followed the guide posted on the course site. Using interrupts for timer
sleep is the crucial part of the design as we need the global state to be 
called within an interrupt and therefore, we cannot use locks to protect the
list of sleeping threads.

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

ADDITIONAL struct thread MEMBERS

struct list waiters; /* list of threads waiting on this thread */

struct semaphore waiters_sema; /* semaphore for list waiters */

struct list_elem waitelem; /* element for inclusion in another thread's waiters list */

int eff_priority; /* for holding higher/donated priority values */

struct lock *blocker; /* lock that blocks this thread */

ADDITIONAL struct semaphore_elem MEMBERS

struct thread *t; /* able to track eff_priority in condvar waitlists */

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

Each thread has a list of threads waiting on it release a lock. 
This is equivalent to the concatention of all lists of the semaphore waiters
for the semas within each lock held by a thread. The effective priority of a
thread is the max of the effective priorities of threads waiting on it.
EG: in both Fig 1 and Fig 2, L's waitlist is just M, 
but in Fig 2 M has effective priority H, so L's effective priority is also H. 

KEY: 
@#: lock
[A-Z]: thread
A <> @: A has lock @ 
@ <- A: A is waiting on lock @
Note: L,M,H threads have the intuitive priorities

Fig 1: Base Case
L <> @1 <- blocker <- M <> @2
|	 |
|    V
|	holder: L
V
eff_priority: M

Fig 2: Nested Case
L <> @1 <- blocker <- M <> @2 <- blocker <- H
|	 |			 	  |	   |
|	 |		 		  |	   V
|	 |	 			  |	  holder: M
|	 |				  |
|	 |				  V
|	 |				eff_priority: H
|    V
|	holder: L
V
eff_priority: H

Fig 3: Nested Case V2
H <> @1 <- blocker <- L <> @2 <- blocker <- M
|	 |			 	  |	   |
|	 |		 		  |	   V
|	 |	 			  |	  holder: L
|	 |				  |
|	 |				  V
|	 |				eff_priority: M
|    V
|	holder: H
V
eff_priority: H

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

For locks and semaphores, the mechanism is the same. In sema_up,
we find the first element in the sema.waiters with the highest eff_priority.
This preserves fifo given multiple threads with the highest eff_priority in that list.

For condition variables, we use the same logic, except we iterate over the list
of semaphore_elems and access eff_priority via the included thread member.
This works because there is one semaphore_elem per thread waiting on a condvar.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

Suppose a thread H calls lock acquire on a lock @2 held by thread M.
H sees that @2 has a holder, hence begins to donate its priority.
This is handled via a while loop up the 'blocker chain.'
The loop sets M's eff_priority to H then sees that M has a blocking lock, @1.
Since blocking locks must be held, the loop sets @1's holder's (ie, L's) 
eff_priority to H. During this project, we called this process, 'bubbling up'
priority donation, as you only donate in the case where the priority of your blocker
is less than your effective priority. eg, in Fig 3, the while loop only runs one iteration.
Whereas in Fig 2 it runs two.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

Suppose in Fig 2 that L releases @1. L iterates over the list of threads
waiting on @1 and removes them from the list of threads waiting on it.
Then, L resets its eff_priority to L, and iterates over the remaining threads
in its waiting list and increases its eff_priority to the highest eff_priority
in that list. (Notably, if the list is empty eff_priority remains L.)
As discussed above, sema_up will then unblock in fifo manner the highest eff_priority
thread waiting on @1. 

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

There are several race conditions in thread_set_priority(). Firstly, priority/eff_priority is
a value that can be set by any thread (say as it enters the waiting queue for a lock) therefore,
it needs to be protected. Then, if we upgrade the priority of a lock holder, we need to "bubble up"
the priority and this bubbling up requires us to modify the entire eff_priority chain which is
global and therefore, we want this modification to be atomic. 

Individual locks for each thread's priority values is insufficient because we would like to
modify the entire chain atomically to properly reflect the priority dependencies. If there were
only a lock on each thread's value than it is possible the chain could change before
full donation is complete.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We considered a recursive list of locks held. The problem was that if
multiple threads with multiple priorities wait on a thread, we lose the ability to properly
reset priorites. With a list of waiting threads, we can iterate over the threads when 
priority requires recomputation (eg, on lock release).

We settled on a list of waiting threads because we were unable to come up with a clean way
to track locks held by a thread (in particular, we could not edit synch.h).
We had thought about giving locks a list_elem member along with an eff_priority member.

We ultimately chose to use the waiting thread list and blocking lock implementation,
as the blocking lock will not change (suppose M holds @1 and L, H wait on @1. If we had
used a blocking thread and L was waiting first, we would have to change the value of L's
blocking thread, as H would get @1 first.) With our implementation, we simply access
the holder value of the blocking lock when we donate priority.

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

ADDITIONAL struct thread MEMBERS:

fp_t recent_cpu; /*holder of recent_cpu for this thread*/

int nice; /* The nice value for calculating priority *.

ADDITIONAL GLOBAL VARIABLES:

static struct list mlfqs_list[64]; /*array with one priority queue per index */

static struct semaphore mlfqs_sema; /*lock to protect above array */

static fp_t load_avg; /*load average holder for the system */

static struct semaphore load_sema; /* protect the load_sema global */

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59    A
 4      4   0   0  62  61  59    A
 8      8   0   0  61  61  59    B
12      8   4   0  61  60  59    A
16     12   4   0  60  60  59    B
20     12   8   0  60  59  59    A
24     16   8   0  59  59  59    C
28     16   8   4  59  59  58    B
32     16  12   4  59  58  58    A
36     20  12   4  58  58  58    C

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?
One ambiguity that we assumed was the initial load average. We assumed
the initial load average to be 3 because there are three threads competing
for CPU time. We also assumed that each priority acts in a FIFO style. I.e
if two threads have the same priority, say tick 8, B runs before A because it
entered the queue 61 first.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

There are 64 priority queues. Iterating through that list takes time to
a) recalculate necessary values such as load_avg, recent_cpu and priority
FOR EVERY THREAD and
b) iterating over the ready queues to find the next thread to run
thus, putting too much code in interrupt context (say, a timer interrupt)
will give less time in each time slice for actual code to run, resulting 
in code that runs in more ticks than it needs. Code outside of interrupt
context doesn't have this limitation however, you will need to secure
global values via locks which will decrease performance in this case.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

Currently, we have one large lock for all 64 queues. One reason to take
this approach is because it is easier to maintain, reduces code complexity,
and avoids deadlock. However, for operating systems, the chief concern is
resource efficiency so for future designs, we might want to have individual
queue locks. 

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

We used the included fixed-point library which provided simple functions
for floating point math and conversion between int and floating point
and vice versa. 

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

We felt that the assignment was fairly appropriate, if a shade on the long side.
In particular, we felt it was a little bizarre to have the two different scheduling
algorithms implemented in the same code base, as it required some awkward
conditional branches within the code. It was also a bit of a jarring change of pace
to work on MLFQS after we had worked on priority donation.

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

Yes, There seems to be a big tradeoff in scheduling between priority
and avoiding thread starvation. Implementing this project (especially mlfqs)
gave us good insight on how to balance this tradeoff using multi-layered queues
and exponential decay functions for priority calculation.

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

Perhaps some more elucidating examples or descriptions on how the list
implementation works. While we came to appreciate this implementation
(in particular, no need for allocating memory), a good deal of time
was spent wrapping our heads around these lists.

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?

It would have been nice to be able to edit synch.h. We felt we could have had a cleaner
donation implementation if we could have given each lock a priority value and
each thread had a list of held locks. This restriction felt very unnecessary.

We thought that it was a little bizarre to have condvars in the implemenation.
The changes were fairly minor, but it felt like a strange and unnecessary addition
when most of the work went into the semaphore and lock implementations.